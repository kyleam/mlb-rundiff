---
title: "Evaluation of a simple single-year team ability model"
---

```{r setup, include = FALSE}
source("setup.R")

library(dplyr)
library(forcats)
library(ggplot2)
library(readr)
library(stringr)
library(tidyr)

library(rstan)
library(bayesplot)
color_scheme_set(c("purple"))
theme_set(theme_minimal())
```

## Single-year estimation of team ability

Eventually I want to estimate the association between a team having
jet lag and some measure of their tendency to win a game.  As a
starting point, I remembered that Andrew Gelman
had [some][p1] [posts][p2] on estimating the ability of teams in the
World Cup teams.  The response in that model is the score
differential: goals in that case, runs my case.  As Gelman discusses,
this isn't a generative model because it makes continuous predictions
for discrete data, but it may be good enough for my purposes.

[p1]: http://andrewgelman.com/2014/07/13/stan-analyzes-world-cup-data/
[p2]: http://andrewgelman.com/2014/07/15/stan-world-cup-update/

Below is a model that closely follows the World Cup model.

```{r, cache.extra = file.info("../models/scorediff-oneseason.stan"), echo = FALSE, comment = NA, collapse = FALSE}
writeLines(readLines("../models/scorediff-oneseason.stan"))
```

There are two main differences from the World Cup model:

  * I haven't yet bothered to put the response (difference in runs) on
    the square-root scale.

  * I don't include a prior estimate for a team's ability.

And while I'm labeling one team as "home" and one as "away", the model
isn't treating them any differently (e.g., I haven't yet tried to add
a parameter for home-field advantage).

## MCMC diagnostics

I've fit this model using data for the 2011 season.  I'll look at a
few diagnostic plots using the [bayesplot] package.

[bayesplot]: http://mc-stan.org/users/interfaces/bayesplot.html

```{r fit, cache.extra = file.info("../models/scorediff-oneseason-2011-samples_1.csv")}
fit <- read_stan_csv(paste0("../models/scorediff-oneseason-2011-samples_", 1:4, ".csv"))
```

The R-hats and effective sample sizes look OK.  Both sets of
statistics look better for the ability parameters than they do for the
variance parameters (the two outliers in each plot).

```{r rhat_plot, dependson = "fit", out.width = "50%", fig.width = 4.28, fig.align = "default"}
mcmc_rhat_hist(rhat(fit)) +
    theme(panel.grid = element_blank(),
          legend.position = "left")

mcmc_neff_hist(neff_ratio(fit)) +
    theme(panel.grid = element_blank())
```

```{r samps, dependson = "fit"}
samps <- extract(fit, permuted = FALSE)
```

The autocorrelation across iterations looks OK.  As expected from the
effective sample size ratios, the variance parameters show higher
autocorrelation than the ability parameters do, with $\sigma_a$
showing the highest.

```{r acf_plot_sigma, dependson = "samps"}
mcmc_acf(samps, regex_pars = "sigma") +
    theme(panel.grid = element_blank())
```

```{r acf_plot_a, dependson = "samps", out.width = "95%", fig.width = 8.14}
mcmc_acf(samps, pars = paste0("a[", 1:6, "]")) +
    theme(panel.grid = element_blank())
```

The traces look OK

```{r sigma_trace_plots, dependson = "samps", fig.asp = 0.35}
mcmc_trace(samps, regex_pars = "sigma") +
    theme(panel.grid = element_blank())
```

```{r sigma_a_plots, dependson = "samps", out.width = "95%", fig.width = 8.14, fig.asp = 0.75}
mcmc_trace(samps, regex_pars = "^a", facet_args = list(ncol = 5)) +
    theme(legend.position = "none", panel.grid = element_blank(),
          axis.text.x = element_blank())
```

## Ability estimates

The order of team ability estimates should largely follow the final
rankings for the 2011 season.

```{r gl, cache.extra = file.info("lag/log-with-lags-cleaned.csv")}
gl <- read_csv("../lag/log-with-lags-cleaned.csv")
```

```{r team_rank, dependson = "gl"}
(team_rank <- gl %>%
    filter(lubridate::year(date) == 2011) %>%
    mutate(home_win = home_runs_scored > away_runs_scored) %>%
    gather(team_type, team, away_team:home_team) %>%
    mutate(win = case_when(home_win & team_type == "home_team"  ~ TRUE,
                           home_win ~ FALSE,
                           team_type == "away_team" ~ TRUE,
                           TRUE ~ FALSE)) %>%
    group_by(team) %>%
    summarise(n_wins = sum(win), n_games = n(),
              win_perc = n_wins / n_games) %>%
    arrange(desc(win_perc)))

team_names <- levels(factor(team_rank$team))
```

To get the samples in a more convenient form, I'll re-extract the
samples from the fit, this time with the default setting of `permuted
= TRUE`.

```{r psamps, dependson = "fit"}
psamps <- rstan::extract(fit)
```

```{r avals, dependson = c("psamps", "team_rank")}
avals <- apply(psamps$a, 2,
               function (x) c(mean = mean(x),
                              quantile(x, probs = c(0.1, 0.25, 0.75, 0.9)))) %>%
    reshape2::melt(varnames = c("meas", "team_idx")) %>%
    mutate(meas = str_replace(meas, "([0-9]+)%", "p\\1")) %>%
    spread(meas, value) %>%
    mutate(team = factor(team_names[team_idx], levels = team_rank$team))
```

```{r avals_plot, dependson = "avals", out.width = "50%", fig.width = 4.28, fig.asp = 1.2}
ggplot(avals, aes(x = fct_rev(team))) +
    geom_hline(yintercept = 0, color = "gray90", size = 1) +
    geom_linerange(aes(ymin = p10, ymax = p90), color = "gray30") +
    geom_linerange(aes(ymin = p25, ymax = p75), size = 0.75) +
    geom_point(aes(y = mean)) +
    labs(x = NULL,
         y = "mean ability (with 50% and 80% intervals)",
         title = "Team ability estimates for 2011 season",
         subtitle = "sorted by regular season standings") +
    coord_flip() +
    theme(panel.grid = element_blank())
```

The intervals are wide and largely overlapping, but the ability
estimates seem to approximately track the season standings.  I don't
expect a one-to-one mapping here because the overall win percentages
don't account for actual match-ups or run differentials.

## Posterior predictive check

To generate new responses from the estimated parameters, I'll pick a
few teams to form the match-ups.  The first three are the teams with
the best 2011 records, the last three are the ones with the worst
records, and the middle teams are somewhere in between.

```{r matchups, dependson = c("team_rank")}
ppc_team_names <- c("PHI", "NYA", "MIL", "ARI", "CIN", "OAK", "PIT",
                    "SEA", "MIN", "HOU")
ppc_teams <- match(ppc_team_names, team_names)

matchups <- data.frame(t(utils::combn(ppc_teams, 2)))
names(matchups) <- c("home", "away")
matchups$home_name <- team_names[matchups$home]
matchups$away_name <- team_names[matchups$away]
```

For each of these match-ups, I'll generate a score differential from
samples for each iteration and then summarize the response across the
iterations by constructing the 95% intervals for each match-up.

```{r scorediff_sim, dependson = c("matchups", "psamps")}
df <- 7

n_games <- nrow(matchups)
n_iter <- nrow(psamps$sigma_y)
scorediff_sim <- array(NA, c(n_iter, n_games))
set.seed(16125)
for (i in 1:n_iter){
    scorediff_sim[i,] <- psamps$a[i, matchups$home] - psamps$a[i, matchups$away] +
         rt(n_games, df) * psamps$sigma_y[i]
}

bounds <- apply(scorediff_sim, 2, function(x) quantile(x, c(0.025, 0.975)))
bounds <- as.data.frame(t(bounds))
names(bounds) <- c("upper", "lower")

result <- cbind(matchups, bounds)  %>%
    as_tibble() %>%
    mutate(matchup = paste(home_name, "-", away_name),
           matchup = factor(matchup, levels = matchup))
```

I also have to pull together the observed run differentials for these
match-ups.  This requires a little bit of work because the simulations
put the team with the better record as the "home" team.

```{r observed, dependson = c("team_names", "scorediff_sim", "result")}
observed <- gl %>%
    filter(lubridate::year(date) == 2011,
           home_team %in% team_names,
           away_team %in% team_names) %>%
    select(home_team, away_team, home_runs_scored, away_runs_scored) %>%
    mutate(flip = match(home_team, team_names) > match(away_team, team_names),
           matchup = ifelse(flip,
                            paste(away_team, "-", home_team),
                            paste(home_team, "-", away_team)),
           matchup = factor(matchup, levels = levels(result$matchup)),
           scorediff = ifelse(flip,
                              away_runs_scored - home_runs_scored,
                              home_runs_scored - away_runs_scored)) %>%
    filter(!is.na(matchup))
```

Now, we can overlay the observed run differentials over the prediction
intervals.  The intervals aren't shown if the match-up didn't occur
during the 2011 season.

```{r ppc_plot, dependson = c("observed")}
result %>%
    filter(matchup %in% observed$matchup) %>%
    ggplot() +
    geom_hline(yintercept = 0, color = "#e5cce5") +
    geom_linerange(aes(x = fct_rev(matchup), ymin = lower, ymax = upper),
                   alpha = 0.8) +
    geom_point(aes(x = matchup, y = scorediff),
               fill = NA, color = "gray30", shape = 21,
               data = observed) +
    scale_y_continuous(limits = c(-13, 13),
                       minor_breaks = -13:13) +
    coord_flip() +
    labs(title = "Run differential",
         subtitle = "compared to the match-up's 95% predictive interval") +
    theme(axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          panel.grid.major.y = element_blank(),
          axis.text.y = element_text(hjust = 0))
```

Well, that seems OK in the sense that 115 of the 122 points (close to
95%) fall within the intervals.

Is this model useful going forward?  I don't know.  If making
predictions from this model were the goal, the intervals are
unhelpfully large. ("Oh, that team's likely to win by 9 or lose by 7?
You don't say.")  And these wide intervals are unsurprising given that
the response is based on a single-season ability estimate for each
team.

Instead, my goal is to find a relatively interpretable model that I
can extend to estimate the association of jet lag with a team's
success in a game.  In that case, the uncertainty of the team-to-team
match-up seems like a good thing to build on.

In order to do that, here are the next steps I'm considering:

  * Introduce terms for different lag effects.

  * Extend the model to handle multiple years.

  * Add a term for a home-team effect.

  * See how the model looks when run differential is put on the
    square-root scale.

  * Break up the ability estimates across the season, similar to Milad
    Kharratzadeh's model of [goal][mk1] [differentials][mk2] in the
    English Premier League.

[mk1]: http://andrewgelman.com/2017/05/17/using-stan-week-week-updating-estimated-soccer-team-abilites/
[mk2]: https://github.com/milkha/EPL

------------------------------------------------------------------------

```{r session_info, echo = FALSE, results = "asis"}
source("session-info.R")
```
